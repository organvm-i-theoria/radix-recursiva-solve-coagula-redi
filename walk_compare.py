"""Utility for scanning digest files for protocol-related keywords.

The script walks ``ARCHIVAL_STACK`` for ``PR*_THREAD_DIGEST.md`` files, reads
the thread table contained in each, and records rows whose titles mention one
of the target keywords.  Results are appended to ``keyword_matches.csv`` with
the thread ID, digest path, title, and the keyword that triggered the match.

The behaviour can be customised via command-line arguments.  Run

``python walk_compare.py --help``

for details.
"""

from __future__ import annotations

import argparse
import csv
import re
from pathlib import Path
from typing import Iterable, List, Tuple, Optional

DEFAULT_KEYWORDS = ["protocol", "SOP", "process", "naming"]


def _match_keyword(title: str, keywords: Iterable[str]) -> Optional[str]:
    """Return the first keyword found in ``title`` or ``None``."""

    lower_title = title.lower()
    for kw in keywords:
        kw_lower = kw.lower()
        if kw_lower in lower_title:
            return kw_lower
    return None


def _scan_file(path: Path, keywords: Iterable[str]) -> List[Tuple[str, str, str, str]]:
    """Return matches from a single digest file.

    Each match is a tuple of ``(thread_id, digest_path, title, keyword)``.
    """

    row_re = re.compile(r"^\|\s*(TH\d+)\s*\|\s*([^|]+?)\s*\|")
    matches: List[Tuple[str, str, str, str]] = []
    with path.open("r", encoding="utf-8") as f:
        for line in f:
            m = row_re.match(line)
            if not m:
                continue
            thread_id, title = m.groups()
            kw = _match_keyword(title, keywords)
            if kw:
                matches.append((thread_id, str(path), title.strip(), kw))
    return matches


def scan_digests(
    digest_dir: Path = Path("ARCHIVAL_STACK"),
    report_path: Path = Path("keyword_matches.csv"),
    keywords: Iterable[str] = DEFAULT_KEYWORDS,
) -> None:
    """Scan ``digest_dir`` for keyword matches and append them to ``report_path``."""

    digest_files = sorted(Path(digest_dir).glob("PR*_THREAD_DIGEST.md"))
    matches: List[Tuple[str, str, str, str]] = []
    for path in digest_files:
        matches.extend(_scan_file(path, keywords))

    # track existing entries to avoid duplicates
    existing = set()
    if report_path.exists():
        with report_path.open(newline="", encoding="utf-8") as csvfile:
            reader = csv.DictReader(csvfile)
            for row in reader:
                existing.add((row["thread_id"], row["digest_path"]))

    write_header = not report_path.exists()
    with report_path.open("a", newline="", encoding="utf-8") as csvfile:
        fieldnames = ["thread_id", "digest_path", "title", "keyword"]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        if write_header:
            writer.writeheader()
        for thread_id, dpath, title, keyword in matches:
            key = (thread_id, dpath)
            if key not in existing:
                writer.writerow(
                    {
                        "thread_id": thread_id,
                        "digest_path": dpath,
                        "title": title,
                        "keyword": keyword,
                    }
                )
                existing.add(key)


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Scan digest files for protocol-related keywords."
    )
    parser.add_argument(
        "--digest-dir", default="ARCHIVAL_STACK", help="Directory containing digest files"
    )
    parser.add_argument(
        "--report",
        default="keyword_matches.csv",
        help="CSV file to append matches to",
    )
    parser.add_argument(
        "--keywords",
        default=",".join(DEFAULT_KEYWORDS),
        help="Comma-separated list of keywords to search for",
    )
    args = parser.parse_args()

    keywords = [k.strip() for k in args.keywords.split(",") if k.strip()]
    scan_digests(Path(args.digest_dir), Path(args.report), keywords)


if __name__ == "__main__":
import csv
from pathlib import Path

OUTPUT_FILE = Path("thread_protocol_index.csv")


def find_thread_files():
    """Return all markdown or CSV files containing "THREAD" in their name.

    The search spans the entire repository to be exhaustive. The output CSV
    generated by this script is excluded to prevent self-referencing.
    """
    patterns = ["*THREAD*.md", "*THREAD*.csv"]
    files = set()
    for pattern in patterns:
        files.update(Path('.').rglob(pattern))
    # Avoid including the output file itself
    return sorted(p for p in files if p.resolve() != OUTPUT_FILE.resolve())
import hashlib
from io import BytesIO
from pathlib import Path
import zipfile

KEYWORDS = ['protocol', 'sop', 'naming', 'process']
IGNORE = {'walk_report.csv'}

def analyze(root='.'):
    root_path = Path(root).resolve()
    report = []
    max_depth = [0]  # mutable holder

    def update_depth(depth: int) -> None:
        if depth > max_depth[0]:
            max_depth[0] = depth

    def handle_file(display_path: str, name: str, data: bytes, depth: int) -> None:
        name_lower = name.lower()
        name_matches = [kw for kw in KEYWORDS if kw in name_lower]
        text = data.decode('utf-8', errors='ignore').lower()
        content_matches = [kw for kw in KEYWORDS if kw in text]
        mismatch = sorted(set(name_matches) ^ set(content_matches))
        sha256 = hashlib.sha256(data).hexdigest()
        report.append({
            'path': display_path,
            'name_matches': ';'.join(name_matches),
            'content_matches': ';'.join(content_matches),
            'mismatch': ';'.join(mismatch),
            'sha256': sha256,
        })
        update_depth(depth)

    def scan_zip(zf: zipfile.ZipFile, base: str, base_depth: int) -> None:
        for info in zf.infolist():
            if info.is_dir():
                continue
            data = zf.read(info.filename)
            inner_path = f"{base}!{info.filename}"
            inner_depth = base_depth + len(Path(info.filename).parts)
            handle_file(inner_path, info.filename, data, inner_depth)
            if info.filename.lower().endswith('.zip'):
                with zipfile.ZipFile(BytesIO(data)) as inner_zip:
                    scan_zip(inner_zip, inner_path, inner_depth)

    for path in root_path.rglob('*'):
        if path.name in IGNORE or not path.is_file():
            continue
        rel_path = path.relative_to(root_path)
        depth = len(rel_path.parts)
        if path.suffix.lower() == '.zip':
            try:
                with zipfile.ZipFile(path) as zf:
                    scan_zip(zf, str(rel_path), depth)
            except zipfile.BadZipFile:
                data = path.read_bytes()
                handle_file(str(rel_path), path.name, data, depth)
        else:
            data = path.read_bytes()
            handle_file(str(rel_path), path.name, data, depth)

    return report, max_depth[0]

def main() -> None:
    report, max_depth = analyze('.')
    print(f'Max depth: {max_depth}')
    out_file = Path('walk_report.csv')
    with out_file.open('w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(
            f,
            fieldnames=['path', 'name_matches', 'content_matches', 'mismatch', 'sha256'],
            lineterminator='\n',
        )
        writer.writeheader()
        writer.writerows(report)
    print(f'Wrote {len(report)} entries to {out_file}')
import glob
from pathlib import Path

# Files to parse
CROSSWALK_FILE = Path('THREAD_PROJECT_CROSSWALK.md')
DIGEST_GLOB = 'ARCHIVAL_STACK/PR*_THREAD_DIGEST.md'
OUTPUT_FILE = Path('thread_protocol_index.csv')

def parse_markdown_table(file_path):
    """Parse markdown table rows for thread entries.

    Expects rows formatted like:
    | THxxx | Title | timestamp | ... |
    Returns list of dicts with thread_id, title, timestamp.
    """
    rows = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line.startswith('| TH'):
                # remove leading/trailing '|'
                parts = [p.strip() for p in line.strip('|').split('|')]
                if len(parts) >= 3:
                    rows.append(
                        {
                            "thread_id": parts[0],
                            "title": parts[1],
                            "timestamp": parts[2],
                            "source_file": str(file_path),
                        }
                    )
    return rows


def parse_csv_table(file_path):
    """Parse CSV files for thread entries."""
    rows = []
    with open(file_path, newline="", encoding="utf-8") as f:
        reader = csv.reader(f)
        headers = next(reader, None)
        for row in reader:
            if row and row[0].startswith("TH"):
                title = row[1] if len(row) > 1 else ""
                timestamp = row[2] if len(row) > 2 else ""
                rows.append(
                    {
                        "thread_id": row[0],
                        "title": title,
                        "timestamp": timestamp,
                        "source_file": str(file_path),
                    }
                )
                    rows.append({
                        'thread_id': parts[0],
                        'title': parts[1],
                        'timestamp': parts[2],
                        'source_file': str(file_path)
                    })
    return rows

def build_index():
    rows = []
    for file_path in find_thread_files():
        if file_path.suffix.lower() == ".md":
            rows.extend(parse_markdown_table(file_path))
        elif file_path.suffix.lower() == ".csv":
            rows.extend(parse_csv_table(file_path))

    rows.sort(key=lambda r: (r["thread_id"], r["source_file"]))

    with open(OUTPUT_FILE, "w", newline="", encoding="utf-8") as csvfile:
        fieldnames = ["thread_id", "title", "timestamp", "source_file"]
    # parse crosswalk
    if CROSSWALK_FILE.exists():
        rows.extend(parse_markdown_table(CROSSWALK_FILE))
    # parse digest files
    for digest_file in sorted(glob.glob(DIGEST_GLOB)):
        rows.extend(parse_markdown_table(Path(digest_file)))
    # write CSV
    with open(OUTPUT_FILE, 'w', newline='', encoding='utf-8') as csvfile:
        fieldnames = ['thread_id', 'title', 'timestamp', 'source_file']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(rows)

def main():
    build_index()
    print(f"Wrote {OUTPUT_FILE}")

if __name__ == '__main__':
    main()
